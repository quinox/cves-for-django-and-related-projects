#!/usr/bin/env python3
from html.parser import HTMLParser
from pathlib import Path
import argparse
import json
import logging
import os
import sys
import unittest
import urllib.request

DESCRIPTION = 'Downloads Django CVE information to JSON'
NAME = os.path.basename(__file__).rsplit('.', 1)[0]


logger = logging.getLogger(NAME)

CVE_MAIN_PAGE = 'https://docs.djangoproject.com/en/dev/releases/security/'
CVE_SUBPAGE_PREFIX = 'https://www.djangoproject.com/weblog/'


class MainPageParser(HTMLParser):
    EXPECT_DATE = 'date'
    EXPECT_CVE = 'cve'
    EXPECT_PILCROW1 = '_pilcrow1'
    EXPECT_SUMMARY = 'summary'
    EXPECT_FULL_AHREF = 'link'
    EXPECT_NOTHING_MORE = '_???'

    EXPECTING = [
        EXPECT_DATE,
        EXPECT_CVE,
        EXPECT_PILCROW1,
        EXPECT_SUMMARY,
        EXPECT_FULL_AHREF,
    ]

    # The page isn't 100% uniform, we should ignore a few pesky divs:
    BAD_DIV_IDS = [
        's-archive-of-security-issues',  # intro text
        's-issues-prior-to-django-s-security-process',  # intro text
        's-issues-under-django-s-security-process',  # intro text
        # Manually added to the repo:
        's-august-16-2006-cve-2007-0404',  # odd format
        's-december-10-2012-no-cve-1',  # No CVE 1
        's-id22',  # No CVE 1
        's-december-10-2012-no-cve-2',  # No CVE 2
        's-id23',  # No CVE 2
        's-february-19-2013-no-cve',  # No CVE 3
        's-id24',  # No CVE 3
        's-february-19-2013-cve-2013-1664-cve-2013-1665',  # Two CVEs 1
        's-id25',  # Two CVEs 1
        's-august-18-2015-cve-2015-5963-cve-2015-5964',  # Two CVEs 2
        's-id52',  # Two CVEs 2
    ]

    def __init__(self):
        super().__init__()
        self.exploits = []
        self.partial_exploit = None

    def handle_starttag(self, tag, attrs):
        if not self.expecting:
            if tag == 'div' and dict(attrs).get('class') == 'section':
                element_id = dict(attrs).get('id')
                if element_id in self.BAD_DIV_IDS:
                    logger.debug("Ignoring bad div with ID %s", element_id)
                    return
                logger.debug("Encountered an interesting start tag: %s (id %s)", tag, element_id)
                self.partial_exploit = {}
        elif self.expecting == self.EXPECT_NOTHING_MORE:
            self.finalize_partial_exploit()
        elif self.expecting == self.EXPECT_FULL_AHREF:
            if tag == 'a':
                logger.debug('Encountered the ahref: %s', tag)
                self.partial_exploit[self.EXPECT_FULL_AHREF] = dict(attrs).get('href')

    def finalize_partial_exploit(self):
        # Cleanup
        for key in list(self.partial_exploit.keys()):
            if key.startswith('_'):
                del self.partial_exploit[key]
        try:
            self.partial_exploit[self.EXPECT_SUMMARY] = self.partial_exploit[self.EXPECT_SUMMARY].rstrip(' .')
        except KeyError:
            pass

        # Storing for posterity
        if self.partial_exploit['link'].startswith(CVE_SUBPAGE_PREFIX):
            self.exploits.append(self.partial_exploit)
        else:
            logger.debug(
                "On closer inspection this doesn't seem to be a CVE, the link points to %s",
                self.partial_exploit['link'],
            )
        self.partial_exploit = None

    def handle_data(self, raw_data):
        if self.expecting:
            data = raw_data.replace('\n', ' ').strip('-\n ')
            if not data:
                return
            logger.debug(f"Expected %s: %r", self.expecting, data)
            if self.expecting == self.EXPECT_FULL_AHREF:  # this only happens when the summary isn't done yet
                self.partial_exploit[self.EXPECT_SUMMARY] += f" {data}"
            else:
                self.partial_exploit[self.expecting] = data

    @property
    def expecting(self):
        if self.partial_exploit is None:
            return None

        for key in self.EXPECTING:
            if key not in self.partial_exploit:
                return key

        return self.EXPECT_NOTHING_MORE


class DetailPageParser(HTMLParser):

    DOWNLOAD_PREFIXES = ['https://www.djangoproject.com/m/releases/']

    def __init__(self):
        super().__init__()
        # it's possible we encounter the same version information on the page multiple times:
        # it's all the same, so we'll use a set
        self.safe_versions = set()

    def handle_starttag(self, tag, _attrs):
        attrs = dict(_attrs)
        if tag != 'a':
            return
        href = attrs.get('href')
        if not href:
            return

        # A few different formats, depending on the age of the notice

        if href.startswith('https://www.djangoproject.com/m/releases/'):
            # https://www.djangoproject.com/m/releases/2.2/Django-2.2.16.tar.gz
            elems = href.split('/')
            main_version = elems[5]
            full_version = elems[6].strip('Django-.tar.gz')
            self.safe_versions.add((main_version, full_version))
            return

        if href.startswith('/download/') and href.endswith('/tarball/'):
            # /download/0.95.1/tarball/
            elems = href.split('/')
            full_version = elems[2]
            main_version = full_version.rpartition('.')[0]
            self.safe_versions.add((main_version, full_version))
            return

        if href.startswith('http://www.djangoproject.com/download/') and href.endswith('/tarball/'):
            # http://www.djangoproject.com/download/0.96.4/tarball/
            elems = href.split('/')
            full_version = elems[4]
            main_version = full_version.rpartition('.')[0]
            self.safe_versions.add((main_version, full_version))
            return


class Processor:
    def __init__(self, options):
        super().__init__()
        self.options = options
        self.download_cache = {}

    def run(self, *args, **kwargs):
        logger.warning("Downloading main page...")
        all_cves = self.download_cves_overview()
        logger.warning("Found %s CVEs", len(all_cves))
        logger.warning("Download sub pages...")

        all_enriched_cves = []
        for cve in all_cves:
            enriched = self.get_cve_details(cve)
            if not enriched['versions']:
                logger.warning("%s: no known safe versions. Please enrich this file by hand", cve['cve'])
            all_enriched_cves.append(enriched)

        logger.warning("Writing combined file.")
        combined_file = self.options.output_dir / "combined.json"
        with combined_file.open('w') as handle:
            json.dump(all_enriched_cves, handle, indent=2)

        logger.warning("All done.")

    def get_html(self, link):
        logger.info('Downloading %s', link)
        html = None
        # In-memory cache, always active
        try:
            return self.download_cache[link]
        except KeyError:
            pass

        # On-drive cache, sometimes active
        cachefile = None
        if self.options.cache_dir:
            cachefile = self.options.cache_dir / link.replace('/', '_').replace('\\', '_')
            if cachefile.exists():
                with cachefile.open() as handle:
                    logger.info('Found in the cache folder')
                    return handle.read()

        headers = {'User-Agent': '{}/1.0 ({})'.format(NAME, __name__)}

        req = urllib.request.Request(link, headers=headers)
        html = urllib.request.urlopen(req).read().decode('utf-8')

        self.download_cache[link] = html
        if cachefile:
            with cachefile.open('w') as handle:
                handle.write(html)

        return html

    def download_cves_overview(self):
        html = self.get_html(CVE_MAIN_PAGE)

        parser = MainPageParser()
        parser.feed(html)

        return parser.exploits

    def get_cve_details(self, cve):
        cve_file = self.options.output_dir / f"{cve['cve']}.json"
        if cve_file.exists():
            logger.debug("I already have %s", cve['cve'])
            with cve_file.open() as handle:
                return json.load(handle)

        html = self.get_html(cve['link'])

        logger.info(cve)

        parser = DetailPageParser()
        parser.feed(html)

        combined = {}
        combined.update(cve)
        combined['versions'] = sorted(list(parser.safe_versions))
        with cve_file.open('w') as handle:
            json.dump(combined, handle)

        return combined


def directory_create_if_not_exists(value):
    path = Path(value)
    if not path.exists():
        path.mkdir()
    if not path.is_dir():
        raise argparse.TypeError("The given path should be a directory")
    return path


def run_cli():
    parser = argparse.ArgumentParser(description=DESCRIPTION)
    parser.add_argument(
        '-q',
        '--quiet',
        dest='decrease_verbosity',
        action='count',
        default=0,
        help='Decrease verbosity. Can be used multiple times.',
    )
    parser.add_argument(
        '-v',
        '--verbose',
        dest='increase_verbosity',
        action='count',
        default=0,
        help='Increase verbosity. Can be used multiple times.',
    )
    parser.add_argument('--cache-dir', type=directory_create_if_not_exists, required=False)
    parser.add_argument('--output-dir', type=directory_create_if_not_exists, required=True)

    args = parser.parse_args()

    calculated_log_level = logging.WARNING - (10 * args.increase_verbosity) + (10 * args.decrease_verbosity)
    desired_log_level = max(logging.DEBUG, min(logging.CRITICAL, calculated_log_level))
    logformat = '%(asctime)s %(message)s'
    datefmt = '%H:%M:%S'
    logging.basicConfig(level=desired_log_level, format=logformat, datefmt=datefmt)

    logging.debug('Welcome to %s', NAME)
    logging.debug('Options from CLI: %s', args)

    processor = Processor(args)
    processor.run(args)


if __name__ == '__main__':
    run_cli()
